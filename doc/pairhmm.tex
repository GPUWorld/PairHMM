\documentclass[11pt, oneside]{article}

\title{Enabling high throughput haplotype analysis through hardware acceleration}
\author{Carneiro MO, Biagioli E, Pratt-Szeliga P, Thibault S, Vacek G, Nehab D}

\begin{document}
	\maketitle
	
	\begin{abstract}
	Small variants such as single nucleotide polymorphisms (SNPs) or single base pair insertions or deletions (indel) are not the most difficult task for variant calling algorithms. Latest callsets from the 1000 Genomes project have shown upwards of 99\% sensitivity on those types of variants (cite 1kg). Calling larger indels and short rearrangements (up to 50 bases long) can only be achieved through local denovo assembly of a reference haplotype based on the read data (cite haplotype caller?). For this computation we rely on fast heuristic methods that generate pcandidate haplotypes without trying all possibilties (cite debruijn graphs). Once we have a list of candidate haplotypes, we have to determine which of these haplotypes is the most likely haplotype to explain the read data. For this task we use a paired hidden markov model (PairHMM) that calculates the probabiltity of all alignments of a read to a candidate haplotype, repeating the process for every candidate haplotype against every read and tallying the results. The haplotype with the highest probability across all reads is the chosen one. This PairHMM is responsible for more than 50\% of the runtime of the Haplotype Caller (denovo assembly variant caller available in the Genome Analysis Toolkit) due to it's $N^2$ implementation. Here we show how we can harness the parallelism of graphics processing units (GPU) and field-programmable gate arrays (FPGA) to reduce the runtime by ~300 fold. This performance improvement not only accelerates the calling process, but also enables projects that need to call tens of thousands of samples which would otherwise be impossible.
	\end{abstract}

	\section{Introduction}
	General introduction on the problem (variant calling), indels and why do we need local reassembly to find variants. (Mauricio)
	\section{Methods}
	\subsection{Mathematical Description of the Problem}
	Full description of the HMM in it's final version. (Mauricio)
	\subsection{CPU implementation}
	Describe the GATK's implementation of the PairHMM. (Mauricio)
	\subsection{FPGA Implementation}
	Describe the FPGA's implementation of the PairHMM. (Scott/George)
	\subsection{CUDA Implementation}
Modern GPUs are massively parallel computing devices
composed of multiple independent multiprocessors, each
capable of executing the same operation on multiple data
elements simultaneously. The architecture is designed for
high throughput, rather than for low latency.  For best
performance, the computation must be divided according to
this two-level hierarchy: a number of independent jobs large
enough to keep all multiprocessors busy (inter-block
parallelism), each job to be executed by collaborating cores 
that perform computations in tandem (intra-block
parallelism).

To this end, we enqueue as many pairs as will fit into the
GPU memory before launching a persistent kernel.  Each
computational block loops obtaining pairs from the queue
until it is empty. This setup supplies the necessary
inter-block parallelism.  Due to the restricted amount of
shared-memory available, and to enable a larger number of
resident blocks, we break the traversal of matrices~$X$, $Y$
and~$M$ into horizontal bands. Finally, since diagonal
entries are independent from one another, we evaluate
diagonal elements in parallel thereby exposing the required
intra-block parallelism. 


	\subsection{Rootbeer Implementation}
	Describe the Rootbeer implementation of the PairHMM. Why is it different from CUDA. (Phil)
	\subsection{Experimental data and methods}
	1000G data reference, talk about the GATK pipeline and how we made these calls and prepare the test data sets. (Mauricio)
	\section{Results}
	Comparison of the different implementations running on NA12878 data.
	How does FPGA and CUDA differ both implementationwise and performance wise? 
	What is the difference from the user's perspective, what do they have to do differently?
	\subsection{experimental results on 1000G data}
	What did we find? New SNPs? New Indels? 
	What were the benefits of calling this data together using the haplotype caller?
	Why is this useful to other medically relevant projects around the world? 

\end{document}

